{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBOsSJLXiORL"
      },
      "source": [
        "<center>\n",
        "    <h1><b>Data-Mining Techniques Assignment</b></h1>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook is part of a university course assignment **'Data-Mining Techniques'**. In this project involves e-commerce analysis using the **Amazon Product Dataset**. The project is devided into two parts:\n",
        "1. Data Exploration & feature engineering, and\n",
        "2. machine learning tasks including clustering, classification, recommendation system, and sentiment analysis.\n",
        "\n",
        "The members of this assignment are shown in the following table\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| Ονοματεπώνυμο    | Αριθμός Μητρώου  |        email         |\n",
        "| :-------------:  | :-------------:  |   :-------------:    |\n",
        "| Ζήκας Αντώνιος   | 1115202100038    | sdi2100038@di.uoa.gr |\n",
        "| Κώτσιλας Σταύρος | 1115201700292    | sdi1700292@di.uoa.gr |\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Data Pre-processing\n",
        "In the first part we will explore the datasets we are going to use and do some pre-processing and analysis on them. We chose to work with the following categories:\n",
        "1. `All_Beauty`\n",
        "2. `Digital_Music`\n",
        "3. `Gift_Cards`\n",
        "4. `Magazine_Subscriptions`\n",
        "5. `Video_Games`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Data Exploration and Feature Engineering\n",
        "### 1. Data Preperation\n",
        "In this section we will extract our datafor the five categories above. We will download the JSON files and we will parse them in order to create the CSV files that we are going to use for the rest of the tasks.\n",
        "\n",
        "#### Downloading the datasets\n",
        "We are going to define a function that will download the datasets for us. Here we are going to use `streamming=True` so we don't download the entire dataset at once, but we will be able to access its contents. This is done for experimenting purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/antonis/Desktop/Data-Mining/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def download_datasets(categories, data_type=\"review\"):\n",
        "    ''' Downloads the specified type of datasets (review or meta) for the given categories. '''\n",
        "    \n",
        "    if data_type not in [\"review\", \"meta\"]:\n",
        "        raise ValueError(\"Invalid data_type. Choose either 'review' or 'meta'.\")\n",
        "    \n",
        "    # Loop through the categories and download the datasets\n",
        "    # using the load_dataset function from the datasets library\n",
        "    datasets = []\n",
        "    for category in categories:\n",
        "        print(f\"Downloading {data_type} dataset for category: {category}\")\n",
        "        dataset = load_dataset(\n",
        "            \"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "            f\"raw_{data_type}_{category}\",\n",
        "            trust_remote_code=True,\n",
        "            streaming=True\n",
        "        )\n",
        "        datasets.append(dataset)\n",
        "    \n",
        "    return datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's download the datasets for the five categories specified above. We will download both **reviews** and **meta** data for the categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading review dataset for category: All_Beauty\n",
            "Downloading review dataset for category: Digital_Music\n",
            "Downloading review dataset for category: Gift_Cards\n",
            "Downloading review dataset for category: Magazine_Subscriptions\n",
            "Downloading review dataset for category: Video_Games\n",
            "Downloading meta dataset for category: All_Beauty\n",
            "Downloading meta dataset for category: Digital_Music\n",
            "Downloading meta dataset for category: Gift_Cards\n",
            "Downloading meta dataset for category: Magazine_Subscriptions\n",
            "Downloading meta dataset for category: Video_Games\n",
            "\n",
            "Datasets downloaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Define the categories to download (can be modified as needed)\n",
        "categories = [\"All_Beauty\", \"Digital_Music\", \"Gift_Cards\", \"Magazine_Subscriptions\", \"Video_Games\"]\n",
        "\n",
        "# Download the review and meta data for the specified categories\n",
        "review_datasets = download_datasets(categories, data_type=\"review\")\n",
        "meta_datasets = download_datasets(categories, data_type=\"meta\")\n",
        "\n",
        "print(\"\\nDatasets downloaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creation of CSV files\n",
        "Finally let's create the corresponding **CSV files** for the datasets and save them locally to use them later. We will also define a function that will handle this for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def construct_csv_files(categories, datasets, max_records=100, output_dir=\"output\"):\n",
        "    ''' Constructs dictionaries for each category from the review or meta datasets and saves them as CSV files. '''\n",
        "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
        "    \n",
        "    categories_dictionaries = {}\n",
        "    for category, dataset in zip(categories, datasets):\n",
        "        csv_path = os.path.join(output_dir, f\"{category}_data.csv\")\n",
        "        \n",
        "        # Check if the CSV file already exists\n",
        "        if os.path.exists(csv_path):\n",
        "            print(f\" - CSV file for category '{category}' already exists at: {csv_path}. Skipping creation.\")\n",
        "            continue\n",
        "        \n",
        "        for i, record in enumerate(dataset['full']):\n",
        "            if i == 0:\n",
        "                dictionary = {key: [] for key in record.keys()}\n",
        "            for key in record.keys():\n",
        "                dictionary[key].append(record[key])\n",
        "            if i == max_records:\n",
        "                break\n",
        "        \n",
        "        # Save the dictionary as a CSV file\n",
        "        df = pd.DataFrame(dictionary)\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\" - CSV file created for category '{category}' at: {csv_path} ({i} records)\")\n",
        "        \n",
        "        categories_dictionaries[category] = dictionary\n",
        "    \n",
        "    return categories_dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Constructing CSV files for review datasets...\n",
            " - CSV file created for category 'All_Beauty' at: ../data/review/All_Beauty_data.csv (1000 records)\n",
            " - CSV file created for category 'Digital_Music' at: ../data/review/Digital_Music_data.csv (1000 records)\n",
            " - CSV file created for category 'Gift_Cards' at: ../data/review/Gift_Cards_data.csv (1000 records)\n",
            " - CSV file created for category 'Magazine_Subscriptions' at: ../data/review/Magazine_Subscriptions_data.csv (1000 records)\n",
            " - CSV file created for category 'Video_Games' at: ../data/review/Video_Games_data.csv (1000 records)\n",
            "\n",
            "Constructing CSV files for meta datasets...\n",
            " - CSV file created for category 'All_Beauty' at: ../data/meta/All_Beauty_data.csv (1000 records)\n",
            " - CSV file created for category 'Digital_Music' at: ../data/meta/Digital_Music_data.csv (1000 records)\n",
            " - CSV file created for category 'Gift_Cards' at: ../data/meta/Gift_Cards_data.csv (1000 records)\n",
            " - CSV file created for category 'Magazine_Subscriptions' at: ../data/meta/Magazine_Subscriptions_data.csv (1000 records)\n",
            " - CSV file created for category 'Video_Games' at: ../data/meta/Video_Games_data.csv (1000 records)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nConstructing CSV files for review datasets...\")\n",
        "review_dictionaries = construct_csv_files(categories, review_datasets, max_records=1000, output_dir=\"../data/review\")\n",
        "\n",
        "print(\"\\nConstructing CSV files for meta datasets...\")\n",
        "meta_dictionaries = construct_csv_files(categories, meta_datasets, max_records=1000, output_dir=\"../data/meta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python (data-mining-env)",
      "language": "python",
      "name": "data-mining-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
