{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBOsSJLXiORL"
   },
   "source": [
    "<center>\n",
    "    <h1><b>Data-Mining Techniques Assignment</b></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a university course assignment **'Data-Mining Techniques'**. This project involves e-commerce analysis using the **Amazon Product Dataset**. The project is devided into two parts:\n",
    "1. Data Exploration & feature engineering, and\n",
    "2. machine learning tasks including clustering, classification, recommendation system, and sentiment analysis.\n",
    "\n",
    "The members of this assignment are shown in the following table\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Ονοματεπώνυμο    | Αριθμός Μητρώου  |        email         |\n",
    "| :-------------:  | :-------------:  |   :-------------:    |\n",
    "| Ζήκας Αντώνιος   | 1115202100038    | sdi2100038@di.uoa.gr |\n",
    "| Κώτσιλας Σταύρος | 1115201700292    | sdi1700292@di.uoa.gr |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of records to work\n",
    "Here we define a constant that describes how many records for each category will be downloaded\n",
    "\n",
    "**Set it to `-1` to download all the available records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORDS_COUNT = 1000 # -1 to download all records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting theme\n",
    "Firstly we will initialize the plotting theme of the graphs. This is an optional and can be be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "theme = 'light' # You can adjust the theme here, options are 'dark' or 'light'\n",
    "if theme == 'dark':\n",
    "    plt.style.use('dark_background')\n",
    "else:\n",
    "    plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Pre-processing\n",
    "In the first part we will explore the datasets we are going to use and do some pre-processing and analysis on them. We chose to work with the following categories:\n",
    "1. `All_Beauty`\n",
    "2. `Digital_Music`\n",
    "3. `Gift_Cards`\n",
    "4. `Magazine_Subscriptions`\n",
    "5. `Video_Games`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Data Exploration and Feature Engineering\n",
    "### 1. Data Preperation\n",
    "In this section we will extract our datafor the five categories above. We will download the JSON files and we will parse them in order to create the CSV files that we are going to use for the rest of the tasks.\n",
    "\n",
    "#### Downloading the datasets\n",
    "We are going to define a function that will download the datasets for us. Here we are going to use `streamming=True` so we don't download the entire dataset at once, but we will be able to access its contents. This is done for experimenting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def download_datasets(categories, data_type=\"review\"):\n",
    "    ''' Downloads the specified type of datasets (review or meta) for the given categories. '''\n",
    "    \n",
    "    if data_type not in [\"review\", \"meta\"]:\n",
    "        raise ValueError(\"Invalid data_type. Choose either 'review' or 'meta'.\")\n",
    "    \n",
    "    # Loop through the categories and download the datasets\n",
    "    # using the load_dataset function from the datasets library\n",
    "    datasets = []\n",
    "    for category in categories:\n",
    "        print(f\"Downloading {data_type} dataset for category: {category}\")\n",
    "        dataset = load_dataset(\n",
    "            \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "            f\"raw_{data_type}_{category}\",\n",
    "            trust_remote_code=True,\n",
    "            streaming=True\n",
    "        )\n",
    "        datasets.append(dataset)\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the datasets for the five categories specified above. We will download both **reviews** and **meta** data for the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading review dataset for category: All_Beauty\n",
      "Downloading review dataset for category: Digital_Music\n",
      "Downloading review dataset for category: Gift_Cards\n",
      "Downloading review dataset for category: Magazine_Subscriptions\n",
      "Downloading review dataset for category: Video_Games\n",
      "Downloading meta dataset for category: All_Beauty\n",
      "Downloading meta dataset for category: Digital_Music\n",
      "Downloading meta dataset for category: Gift_Cards\n",
      "Downloading meta dataset for category: Magazine_Subscriptions\n",
      "Downloading meta dataset for category: Video_Games\n",
      "\n",
      "Datasets downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the categories to download (can be modified as needed)\n",
    "categories = [\"All_Beauty\", \"Digital_Music\", \"Gift_Cards\", \"Magazine_Subscriptions\", \"Video_Games\"]\n",
    "\n",
    "# Download the review and meta data for the specified categories\n",
    "review_datasets = download_datasets(categories, data_type=\"review\")\n",
    "meta_datasets = download_datasets(categories, data_type=\"meta\")\n",
    "\n",
    "print(\"\\nDatasets downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowNotImplementedError",
     "evalue": "Unsupported cast from list<item: struct<hi_res: string, large: string, thumb: string, variant: string>> to struct using function cast_struct",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowNotImplementedError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmeta_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfull\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:2270\u001b[39m, in \u001b[36mIterableDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2267\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m formatter.format_row(pa_table)\n\u001b[32m   2268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2270\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2271\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# no need to format thanks to FormattedExamplesIterable\u001b[39;49;00m\n\u001b[32m   2272\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:1856\u001b[39m, in \u001b[36mFormattedExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1849\u001b[39m     formatter = get_formatter(\n\u001b[32m   1850\u001b[39m         \u001b[38;5;28mself\u001b[39m.formatting.format_type,\n\u001b[32m   1851\u001b[39m         features=\u001b[38;5;28mself\u001b[39m._features \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable.is_typed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1852\u001b[39m         token_per_repo_id=\u001b[38;5;28mself\u001b[39m.token_per_repo_id,\n\u001b[32m   1853\u001b[39m     )\n\u001b[32m   1854\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable.iter_arrow:\n\u001b[32m   1855\u001b[39m     \u001b[38;5;66;03m# feature casting (inc column addition) handled within self._iter_arrow()\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1856\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_batch_to_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:1888\u001b[39m, in \u001b[36mFormattedExamplesIterable._iter_arrow\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1886\u001b[39m         pa_table = pa_table.append_column(column_name, col)\n\u001b[32m   1887\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pa_table.schema != schema:\n\u001b[32m-> \u001b[39m\u001b[32m1888\u001b[39m     pa_table = \u001b[43mcast_table_to_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m key, pa_table\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\datasets\\table.py:2221\u001b[39m, in \u001b[36mcast_table_to_features\u001b[39m\u001b[34m(table, features)\u001b[39m\n\u001b[32m   2215\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[32m   2216\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(table.schema)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mbecause column names don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2217\u001b[39m         table_column_names=table.column_names,\n\u001b[32m   2218\u001b[39m         requested_column_names=\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[32m   2219\u001b[39m     )\n\u001b[32m   2220\u001b[39m arrays = [cast_array_to_feature(table[name], feature) \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features.items()]\n\u001b[32m-> \u001b[39m\u001b[32m2221\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43marrow_schema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\table.pxi:4893\u001b[39m, in \u001b[36mpyarrow.lib.Table.from_arrays\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\table.pxi:1622\u001b[39m, in \u001b[36mpyarrow.lib._sanitize_arrays\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\array.pxi:402\u001b[39m, in \u001b[36mpyarrow.lib.asarray\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\table.pxi:593\u001b[39m, in \u001b[36mpyarrow.lib.ChunkedArray.cast\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\compute.py:410\u001b[39m, in \u001b[36mcast\u001b[39m\u001b[34m(arr, target_type, safe, options, memory_pool)\u001b[39m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    409\u001b[39m         options = CastOptions.safe(target_type)\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcast\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43marr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\_compute.pyx:612\u001b[39m, in \u001b[36mpyarrow._compute.call_function\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\_compute.pyx:407\u001b[39m, in \u001b[36mpyarrow._compute.Function.call\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowNotImplementedError\u001b[39m: Unsupported cast from list<item: struct<hi_res: string, large: string, thumb: string, variant: string>> to struct using function cast_struct"
     ]
    }
   ],
   "source": [
    "for i in meta_datasets[0]['full']:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of CSV files\n",
    "Finally let's create the corresponding **CSV files** for the datasets and save them locally to use them later. We will also define a function that will handle this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def construct_csv_files(categories, datasets, max_records=100, output_dir=\"output\"):\n",
    "    ''' Constructs dictionaries for each category from the review or meta datasets and saves them as CSV files. '''\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "    \n",
    "    categories_dictionaries = {}\n",
    "    for category, dataset in zip(categories, datasets):\n",
    "        csv_path = os.path.join(output_dir, f\"{category}_data.csv\")\n",
    "        \n",
    "        # Check if the CSV file already exists\n",
    "        if os.path.exists(csv_path):\n",
    "            print(f\" - CSV file for category '{category}' already exists at: {csv_path}. Skipping creation.\")\n",
    "            continue\n",
    "        \n",
    "        for i, record in enumerate(dataset['full']):\n",
    "            if i == 0:\n",
    "                dictionary = {key: [] for key in record.keys()}\n",
    "            for key in record.keys():\n",
    "                dictionary[key].append(record[key])\n",
    "            if i == max_records - 1:\n",
    "                break\n",
    "        \n",
    "        # Save the dictionary as a CSV file\n",
    "        df = pd.DataFrame(dictionary)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\" - CSV file created for category '{category}' at: {csv_path} ({len(df)} records)\")\n",
    "        \n",
    "        categories_dictionaries[category] = dictionary\n",
    "    \n",
    "    return categories_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constructing CSV files for review datasets...\n",
      " - CSV file for category 'All_Beauty' already exists at: ../data/raw/review\\All_Beauty_data.csv. Skipping creation.\n",
      " - CSV file for category 'Digital_Music' already exists at: ../data/raw/review\\Digital_Music_data.csv. Skipping creation.\n",
      " - CSV file for category 'Gift_Cards' already exists at: ../data/raw/review\\Gift_Cards_data.csv. Skipping creation.\n",
      " - CSV file for category 'Magazine_Subscriptions' already exists at: ../data/raw/review\\Magazine_Subscriptions_data.csv. Skipping creation.\n",
      " - CSV file for category 'Video_Games' already exists at: ../data/raw/review\\Video_Games_data.csv. Skipping creation.\n",
      "\n",
      "Constructing CSV files for meta datasets...\n"
     ]
    },
    {
     "ename": "ArrowNotImplementedError",
     "evalue": "Unsupported cast from list<item: struct<hi_res: string, large: string, thumb: string, variant: string>> to struct using function cast_struct",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowNotImplementedError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m review_dictionaries = construct_csv_files(categories, review_datasets, max_records=RECORDS_COUNT, output_dir=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/review\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mConstructing CSV files for meta datasets...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m meta_dictionaries = \u001b[43mconstruct_csv_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRECORDS_COUNT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/meta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mconstruct_csv_files\u001b[39m\u001b[34m(categories, datasets, max_records, output_dir)\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m - CSV file for category \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m already exists at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Skipping creation.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfull\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfull\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:2270\u001b[39m, in \u001b[36mIterableDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2267\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m formatter.format_row(pa_table)\n\u001b[32m   2268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2270\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2271\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# no need to format thanks to FormattedExamplesIterable\u001b[39;49;00m\n\u001b[32m   2272\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:1856\u001b[39m, in \u001b[36mFormattedExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1849\u001b[39m     formatter = get_formatter(\n\u001b[32m   1850\u001b[39m         \u001b[38;5;28mself\u001b[39m.formatting.format_type,\n\u001b[32m   1851\u001b[39m         features=\u001b[38;5;28mself\u001b[39m._features \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable.is_typed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1852\u001b[39m         token_per_repo_id=\u001b[38;5;28mself\u001b[39m.token_per_repo_id,\n\u001b[32m   1853\u001b[39m     )\n\u001b[32m   1854\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable.iter_arrow:\n\u001b[32m   1855\u001b[39m     \u001b[38;5;66;03m# feature casting (inc column addition) handled within self._iter_arrow()\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1856\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_batch_to_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\datasets\\iterable_dataset.py:1888\u001b[39m, in \u001b[36mFormattedExamplesIterable._iter_arrow\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1886\u001b[39m         pa_table = pa_table.append_column(column_name, col)\n\u001b[32m   1887\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pa_table.schema != schema:\n\u001b[32m-> \u001b[39m\u001b[32m1888\u001b[39m     pa_table = \u001b[43mcast_table_to_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m key, pa_table\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\datasets\\table.py:2221\u001b[39m, in \u001b[36mcast_table_to_features\u001b[39m\u001b[34m(table, features)\u001b[39m\n\u001b[32m   2215\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[32m   2216\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(table.schema)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mbecause column names don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2217\u001b[39m         table_column_names=table.column_names,\n\u001b[32m   2218\u001b[39m         requested_column_names=\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[32m   2219\u001b[39m     )\n\u001b[32m   2220\u001b[39m arrays = [cast_array_to_feature(table[name], feature) \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features.items()]\n\u001b[32m-> \u001b[39m\u001b[32m2221\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43marrow_schema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\table.pxi:4893\u001b[39m, in \u001b[36mpyarrow.lib.Table.from_arrays\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\table.pxi:1622\u001b[39m, in \u001b[36mpyarrow.lib._sanitize_arrays\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\array.pxi:402\u001b[39m, in \u001b[36mpyarrow.lib.asarray\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\table.pxi:593\u001b[39m, in \u001b[36mpyarrow.lib.ChunkedArray.cast\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\compute.py:410\u001b[39m, in \u001b[36mcast\u001b[39m\u001b[34m(arr, target_type, safe, options, memory_pool)\u001b[39m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    409\u001b[39m         options = CastOptions.safe(target_type)\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcast\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43marr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\_compute.pyx:612\u001b[39m, in \u001b[36mpyarrow._compute.call_function\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\_compute.pyx:407\u001b[39m, in \u001b[36mpyarrow._compute.Function.call\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Data-Mining-venv\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowNotImplementedError\u001b[39m: Unsupported cast from list<item: struct<hi_res: string, large: string, thumb: string, variant: string>> to struct using function cast_struct"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/raw\" # Path to save the CSV files\n",
    "\n",
    "print(\"\\nConstructing CSV files for review datasets...\")\n",
    "review_dictionaries = construct_csv_files(categories, review_datasets, max_records=RECORDS_COUNT, output_dir=f\"{data_path}/review\")\n",
    "\n",
    "print(\"\\nConstructing CSV files for meta datasets...\")\n",
    "meta_dictionaries = construct_csv_files(categories, meta_datasets, max_records=RECORDS_COUNT, output_dir=f\"{data_path}/meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the CSV files to Pandas Dataframes\n",
    "Next we will pre-process and clean our data. We start by loading the loading the CSV files we just saved to **Pandas Dataframes**. This will let us work more conviniently. Let's create a function for that and load the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files(categories, data_path=\"../data\"):\n",
    "    ''' Loads the CSV files for the specified categories into pandas dataframes. '''\n",
    "    \n",
    "    # Initialize a dictionary to hold the dataframes\n",
    "    dataframes = {'review': {}, 'meta': {}}\n",
    "    \n",
    "    # Loop through the categories and load the CSV files into dataframes\n",
    "    for mode in ['review', 'meta']:\n",
    "        for category in categories:\n",
    "            csv_path = os.path.join(data_path, mode, f\"{category}_data.csv\")\n",
    "            if os.path.exists(csv_path):\n",
    "                dataframes[mode][category] = pd.read_csv(csv_path)\n",
    "                print(f\" - Loaded {mode} data for category '{category}' from: {csv_path}\")\n",
    "            else:\n",
    "                print(f\" - CSV file for category '{category}' not found at: {csv_path}. Skipping loading.\")\n",
    "    \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = load_csv_files(categories, data_path=data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframes Visualization\n",
    "Let's have a look at the **review** and **meta** data for the first category: **All_Beauty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beauty_review_df = dataframes['review']['All_Beauty']\n",
    "all_beauty_meta_df = dataframes['meta']['All_Beauty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beauty_review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beauty_meta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "Now that we have our data loaded into pandas dataframe, we can start **pre-processing** them. The **Data Cleaning** that we are going to apply consists of three tasks:\n",
    "1. Handle missing values,\n",
    "2. Normalize prices, and\n",
    "3. Pre-process text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i) Handle Missing Values\n",
    "We start the cleaning procedure by **handling missing values**. We will start by identifying which columns of the datasets have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First search in the review dataframes\n",
    "for category in categories:\n",
    "    dataframe = dataframes['review'][category]\n",
    "    missing_summary = dataframe.isnull().sum()\n",
    "    print(f\"Missing summary in {category} review data:\\n{missing_summary[missing_summary > 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, search in the meta dataframes\n",
    "for category in categories:\n",
    "    dataframe = dataframes['meta'][category]\n",
    "    missing_summary = dataframe.isnull().sum()\n",
    "    print(f\"Missing summary in {category} meta data:\\n{missing_summary[missing_summary > 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see the missing values we are dealing with are refering to the **meta** data of the datasets. The review datasets seems to not have any missing value. As for the meta datasets, it seems to have empty values at columns: **main_category, price, store, bought_together, subtitle, and author**. We are going to deal with these missing values as follows:\n",
    "- `main_category` $\\rightarrow$ fill with the most common value from the other rows\n",
    "- `price` $\\rightarrow$ fill with 0.0\n",
    "- `store` $\\rightarrow$ fill with ''\n",
    "- `bought_together` $\\rightarrow$ fill with empty list []\n",
    "- `subtitle` $\\rightarrow$ fill with ''\n",
    "- `author` $\\rightarrow$ fill with ''\n",
    "\n",
    "Let's define a function that will handle missing values for a dataframe, using the above sceptic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(dataframe):\n",
    "    ''' Handles missing values in a given dataframe. '''\n",
    "    \n",
    "    # Fill 'main_category' with the most common value of the column\n",
    "    dataframe['main_category'] = dataframe['main_category'].fillna(dataframe['main_category'].mode()[0])\n",
    "\n",
    "    # Fill 'price' with the 0.0 value\n",
    "    dataframe['price'] = dataframe['price'].fillna(0.0)\n",
    "\n",
    "    # Fill 'store' with empty string\n",
    "    dataframe['store'] = dataframe['store'].fillna(\"\")\n",
    "\n",
    "    # Fill 'bought_together' with empty list\n",
    "    dataframe['bought_together'] = dataframe['bought_together'].fillna(\"[]\")\n",
    "\n",
    "    # Fill 'title' with empty string\n",
    "    dataframe['title'] = dataframe['title'].fillna(\"\")\n",
    "\n",
    "    # Fill 'subtitle' with empty strings\n",
    "    dataframe['subtitle'] = dataframe['subtitle'].fillna(\"\")\n",
    "\n",
    "    # Fill 'author' with empty string\n",
    "    dataframe['author'] = dataframe['author'].fillna(\"\")\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the above function to all meta dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for all the meta dataframes\n",
    "for dataframe in dataframes['meta'].values():\n",
    "    dataframe = handle_missing_values(dataframe)\n",
    "    dataframes['meta'][category] = dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally let's see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, search in the meta dataframes\n",
    "for category in categories:\n",
    "    dataframe = dataframes['meta'][category]\n",
    "    missing_summary = dataframe.isnull().sum()\n",
    "    print(f\"Missing summary in {category} meta data:\\n{missing_summary[missing_summary > 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see all the missing values have been removed with the previous logic, and we can continue with normalizing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii) Normalizing prices\n",
    "Next we will normalize the prices of the meta dataframes. There are many ways of normalizing numeric values. Some of them are:\n",
    "- Min-Max normalization (Scaling to 0 - 1)\n",
    "- Standardization (Z-score Normalization)\n",
    "- Log Normalization\n",
    "- Currency Normalization (less common)\n",
    "\n",
    "In this notebook we will use **Min-Max normalization (Scaling to 0 - 1)**. Let's define a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_prices(dataframe):\n",
    "    ''' \n",
    "    Normalizes the prices in a given dataframe, using min-max normalization (scaling to [0, 1]),\n",
    "    and creating a new column 'normalized_price' to store the normalized values'.\n",
    "    '''\n",
    "    # Ensure the 'price' column is numeric\n",
    "    dataframe['price'] = dataframe['price'].replace('—', 0.0)\n",
    "    dataframe['price'] = pd.to_numeric(dataframe['price'])\n",
    "    min_price, max_price = dataframe['price'].min(), dataframe['price'].max()\n",
    "    if min_price == max_price:\n",
    "        dataframe['normalized_price'] = 0.0\n",
    "    else:\n",
    "        dataframe['normalized_price'] = (dataframe['price'] - min_price) / (max_price - min_price)\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this function to the meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    dataframe = dataframes['meta'][category]\n",
    "    dataframe = normalize_prices(dataframe)\n",
    "    dataframes['meta'][category] = dataframe\n",
    "    print(f\" - Normalized prices for category '{category}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the meta data of the 'All Beauty' category and check what is the **maximum** and **minimum** price in each dataset related to their normalized values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['meta']['All_Beauty'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    print(f\"{category}: Max price: {dataframes['meta'][category]['price'].max()}\", end=\", \")\n",
    "    print(f\"Min price: {dataframes['meta'][category]['price'].min()}\", end=\", \")\n",
    "    print(f\"Normalized max price: {dataframes['meta'][category]['normalized_price'].max()}\", end=\", \")\n",
    "    print(f\"Normalized min price: {dataframes['meta'][category]['normalized_price'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see the prices have been normalized successfully. We can continue with the text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iii) Pre-processing text\n",
    "We will now begin the text pre-processing of the data. Fistly we will decide witch columns need to be processed. As we saw in the dataframes representations there are two columns in the review data that contain text. These are **`title`** and **`text`**, while in the meta data only the **`title`** column contain text. So we are going to apply text pre-processing as follows:\n",
    "- For the review data $\\rightarrow$ columns `title` and `text`\n",
    "- For the meta data $\\rightarrow$ column `title`\n",
    "\n",
    "The next step is to determine what **pre-processing techniques** will be used. We have decided to apply the following rules on the pre-processing:\n",
    "1. **Lowercase** text,\n",
    "2. Remove **punctuation**,\n",
    "3. **Stemming** and **Lemmatization** of words,\n",
    "4. Remove **URLS** (https://...), **user mentions** (@user123) and **hashtags** (#hashtag_example), and\n",
    "5. Remove **stop-words**\n",
    "\n",
    "We will create a function that will apply the above rules to a single string variable. First we will make sure all the appropriate packages are installed in our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we will define our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    ''' \n",
    "    Preprocesses the text by removing special characters, converting to lowercase, \n",
    "    and removing stop-words. \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # 1. Lowercase the text and remove punctuation\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 2. Apply stemming or lemmatization to words\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    # 3. Remove URLs, user mentions, and hashtags\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "\n",
    "    # 4. Remove stop-words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the cleaning function to the columns mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "output_file_path = \"../data/processed/\"\n",
    "if os.path.exists(output_file_path):\n",
    "    print(f\"Data are already processed and stored in {output_file_path}.\")\n",
    "    dataframes = load_csv_files(categories, data_path=output_file_path)\n",
    "else:\n",
    "    print(\"Pre-processing started\")\n",
    "    os.makedirs(output_file_path)\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(\" - Preprocessing review data...\")\n",
    "    os.makedirs(f\"{output_file_path}/review\", exist_ok=True)\n",
    "    for category, dataframe in dataframes['review'].items():\n",
    "        print(f\"  * Preprocessing {category} review data...\")\n",
    "        dataframe['cleaned_title'] = dataframe['title'].apply(preprocess_text)\n",
    "        dataframe['cleaned_text'] = dataframe['text'].apply(preprocess_text)\n",
    "        dataframe.to_csv(f\"{output_file_path}/review/{category}_data.csv\", index=False)\n",
    "\n",
    "    print(\" - Preprocessing meta data...\")\n",
    "    os.makedirs(f\"{output_file_path}/meta\", exist_ok=True)\n",
    "    for category, dataframe in dataframes['meta'].items():\n",
    "        print(f\"  *  Preprocessing {category} meta data...\")\n",
    "        dataframe['cleaned_title'] = dataframe['title'].apply(preprocess_text)\n",
    "        dataframe.to_csv(f\"{output_file_path}/meta/{category}_data.csv\", index=False)\n",
    "\n",
    "    print(f\"\\nPreprocessing completed in {time.time() - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the pre-processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['review']['All_Beauty'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['meta']['All_Beauty'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ratings and Reviews\n",
    "In this section we will perform some **Exploratory Data Analysis** (EDA) by creating and showing several plots that describe the behaviour of the data according to different tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Product Ratings\n",
    "The first task is to find the distribution between product ratings within each of the 5 categories we have selected. For this task we use `seaborn`, a very popular library for plotting, and creates **5 plots**, one for each category showing the distribution between user ratings for defferent products of a specific category.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a histogram for the rating distribution of each category\n",
    "colors = ['dodgerblue', 'orange', 'green', 'purple', 'red']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot the rating distribution for each category\n",
    "for i, category in enumerate(categories):\n",
    "    sns.histplot(dataframes['review'][category]['rating'], bins=4, kde=True, ax=axes[i], color=colors[i], stat='frequency')\n",
    "    axes[i].set_title(f\"Rating Distribution - {category}\")\n",
    "    axes[i].set_xlabel(\"Rating\")\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "    axes[i].grid(axis='y', color='dimgray', linestyle='dashed', linewidth=0.5)\n",
    "\n",
    "# Hide any unused subplot axes\n",
    "for j in range(len(categories), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above graphs, the **most frequent rating** for each category is **5** and the second one is **1** which makes sence. Finally let's create a bar plot showing the average rating for every category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average rating for each category\n",
    "average_ratings = {}\n",
    "for category in categories:\n",
    "    average_rating = dataframes['review'][category]['rating'].mean()\n",
    "    average_ratings[category] = average_rating\n",
    "\n",
    "# Plot the average ratings\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.bar(average_ratings.keys(), average_ratings.values(), color='green', width=0.6)\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Average Rating\")\n",
    "plt.title(\"Average Ratings by Category\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph we can see that the `Gift_Cards` category has larger average rating than the other 4 categories. The `Digital_music` seems to have almost the same rating as `Gift_Cards`, but `All_Beauty` is having the lowest average rating of all the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Many Reviews but Low Ratings\n",
    "Next we will identify the products with **high number of reviews**, but **low ratings**. Specifically we define the following rules:\n",
    "- **High Reviews Products** are those with `number_of_reviews` being higher than the 75% of the maximum reviews\n",
    "- **Low Rating** is considered to be lower than `2.5`\n",
    "\n",
    "First we will create a function that will calculate and return the products with high number of reviews but low ratings for every category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_reviews_low_ratings_products(dataframes, categories, high_review_threshold=0.75, low_rating_threshold=2.5):\n",
    "    \"\"\"\n",
    "    Identifies products with high reviews and low ratings.\n",
    "    \"\"\"\n",
    "    high_reviews_low_ratings_products = []\n",
    "\n",
    "    # Iterate through each category to find products with high reviews and low ratings\n",
    "    for category in categories:\n",
    "        products_reviews = {product_id: [] for product_id in dataframes['review'][category]['asin']}\n",
    "\n",
    "        # Populate the products_reviews dictionary with reviews, ratings, and product IDs\n",
    "        for _, row in dataframes['review'][category].iterrows():\n",
    "            rating, review, product_id = row['rating'], str(row['cleaned_text']), row['asin']\n",
    "            products_reviews[product_id].append((rating, review))\n",
    "\n",
    "        # Filter products with high reviews and low ratings, and store them in the list\n",
    "        for product_id, value in products_reviews.items():\n",
    "            if len(value) >= high_review_threshold:  # Only consider products with high reviews\n",
    "                average_rating = sum(rating for rating, _ in value) / len(value)\n",
    "                if average_rating <= low_rating_threshold:\n",
    "                    high_reviews_low_ratings_products.append({\n",
    "                        'category': category,\n",
    "                        'product_id': product_id,\n",
    "                        'average_rating': average_rating,\n",
    "                        'reviews': value\n",
    "                    })\n",
    "\n",
    "    return high_reviews_low_ratings_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will construct a function that will return the most common keywords from the products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_keywords(high_reviews_low_ratings_products):\n",
    "    \"\"\"\n",
    "    Extracts and counts common keywords from reviews of products with high reviews and low ratings.\n",
    "    \"\"\"\n",
    "    common_keywords = {}\n",
    "\n",
    "    # Iterate through the products with high reviews and low ratings, and count keywords\n",
    "    for product in high_reviews_low_ratings_products:\n",
    "        reviews = product['reviews']\n",
    "        for _, review in reviews:\n",
    "            words = review.split()\n",
    "            for word in words:\n",
    "                if word not in common_keywords:\n",
    "                    common_keywords[word] = 0\n",
    "                common_keywords[word] += 1\n",
    "\n",
    "    # Sort the keywords by frequency and return the top common keywords\n",
    "    common_keywords = dict(sorted(common_keywords.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    return common_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test these functions and get the product of this type. We remind you that products with high number of reviews are meant to be those with 1000 and more reviews and ratings that are smaller than 2.5 are considered to be low ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH_REVIEW_THRESHOLD = 1000  # Threshold for high reviews (1000 or more)\n",
    "LOW_RATING_THRESHOLD = 2.5  # Threshold for low ratings (2.5 or below)\n",
    "\n",
    "# Get the products with high reviews and low ratings\n",
    "high_reviews_low_ratings_products = get_high_reviews_low_ratings_products(dataframes, categories, HIGH_REVIEW_THRESHOLD, LOW_RATING_THRESHOLD)\n",
    "common_keywords = get_common_keywords(high_reviews_low_ratings_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the products received and plot the most common keywords of these products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of products with high reviews and low ratings: {len(high_reviews_low_ratings_products)}\")\n",
    "high_reviews_low_ratings_df = pd.DataFrame({\n",
    "    'category': [product['category'] for product in high_reviews_low_ratings_products],\n",
    "    'product_id': [product['product_id'] for product in high_reviews_low_ratings_products],\n",
    "    'average_rating': [product['average_rating'] for product in high_reviews_low_ratings_products],\n",
    "    'reviews_count': [len(product['reviews']) for product in high_reviews_low_ratings_products]\n",
    "})\n",
    "high_reviews_low_ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 10 common keywords\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(list(common_keywords.keys())[:10], list(common_keywords.values())[:10], color='orange', width=0.7)\n",
    "plt.xlabel(\"Keywords\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top 10 Common Keywords in Reviews of Products with High Reviews and Low Ratings\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Top 5 Best Selling products\n",
    "Next we will identify the top 5 best selling products of each category. As **best selling** we define those products with the highest number of reviews. We will also represent the features of these products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_best_selling_products = {} # Define a dictionary to keep the best selling products for every category\n",
    "\n",
    "for category in categories:\n",
    "    sorted_meta = dataframes['meta'][category].sort_values(by='rating_number', ascending=False)\n",
    "    top_best_selling_products[category] = sorted_meta.head(5)\n",
    "\n",
    "# Display the top 5 best-selling products for All_Beauty category\n",
    "top_best_selling_products['All_Beauty']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show these products with their feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the features of the top best-selling products\n",
    "for category, products in top_best_selling_products.items():\n",
    "    print(f\"\\nTop 5 Best-Selling Products in {category}:\")\n",
    "    for _, product in products.iterrows():\n",
    "        print(f\" - Product ID: {product['parent_asin']} (Number of Ratings: {product['rating_number']})\")\n",
    "        features = product['features'].split(\"', '\")\n",
    "        if features[0] == \"[]\":\n",
    "            print(\"   > No features available\")\n",
    "        else:\n",
    "            for feature in features:\n",
    "                cleaned_feature = feature.replace(\"['\", \"\").replace(\"']\", \"\")\n",
    "                print(f\"   > Feature: {cleaned_feature}\")\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Products Average Ratings per year\n",
    "The last task is to analyze and visualize how average product rating has evolved within each category over the years. We will start by translating the **`timestamp`** column every category has, witch is represented as a **unix timestamp** in milliseconds, and receiving a more human read representation of it (dd-mm-yyyy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "for category in categories:\n",
    "    dataframes['review'][category]['timestamp_formatted'] = dataframes['review'][category]['timestamp'].apply(\n",
    "        lambda x: datetime.datetime.fromtimestamp(x/1000)\n",
    "    )\n",
    "\n",
    "dataframes['review']['Gift_Cards'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above dataframe, we have successfully created a new column with the formatted timestamp and we can easily work with it by extracting the year, the month, etc. We will now create a function that will return to us the average rating for every year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average rating per year for each category\n",
    "def get_average_rating_per_year(dataframes, categories):\n",
    "    average_ratings_per_year = {}\n",
    "\n",
    "    for category in categories:\n",
    "        df = dataframes['review'][category]\n",
    "        df['year'] = df['timestamp_formatted'].dt.year\n",
    "        average_ratings = df.groupby('year')['rating'].mean().reset_index()\n",
    "        average_ratings_per_year[category] = average_ratings\n",
    "\n",
    "    return average_ratings_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ratings_per_year = get_average_rating_per_year(dataframes, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By executing the code above we have successfully extracted the average rating per year for the products of every category. Let's visualize this trend in a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average ratings per year for each category\n",
    "colors = ['dodgerblue', 'orange', 'green', 'purple', 'red']\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, (category, average_ratings) in enumerate(average_ratings_per_year.items()):\n",
    "    plt.plot(average_ratings['year'], average_ratings['rating'], color=colors[i], marker='o', label=category)\n",
    "\n",
    "min_year = min(min(average_ratings['year']) for average_ratings in average_ratings_per_year.values())\n",
    "max_year = max(max(average_ratings['year']) for average_ratings in average_ratings_per_year.values())\n",
    "\n",
    "min_y = min(min(average_ratings['rating']) for average_ratings in average_ratings_per_year.values())\n",
    "max_y = max(max(average_ratings['rating']) for average_ratings in average_ratings_per_year.values())\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average Rating\")\n",
    "plt.title(\"Average Ratings per Year by Category\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.xticks(range(min_year, max_year+1, 2))\n",
    "plt.ylim(min_y - 0.05*min_y, max_y + 0.05*max_y)\n",
    "plt.grid(color='dimgray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above graph the earliest year production has started is 1997 and it is refered to the **Digital_Music** category. We will also define a function that will return to us the monthly average rating for a specific year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_rating_for_year(dataframes, categories, year):\n",
    "    \"\"\"\n",
    "    Returns the average rating for every month for a specific year across all categories.\n",
    "    \"\"\"\n",
    "    average_ratings = {}\n",
    "\n",
    "    for category in categories:\n",
    "        df = dataframes['review'][category]\n",
    "        df['year'] = df['timestamp_formatted'].dt.year\n",
    "        df['month'] = df['timestamp_formatted'].dt.month\n",
    "\n",
    "        # Filter the dataframe for the specified year\n",
    "        df_year = df[df['year'] == year]\n",
    "\n",
    "        # Calculate the average rating per month\n",
    "        monthly_average = df_year.groupby('month')['rating'].mean().reset_index()\n",
    "        average_ratings[category] = monthly_average\n",
    "\n",
    "    return average_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we test the function that we have just created for the year **2021** and we represent the results also in a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "average_ratings_2021 = get_average_rating_for_year(dataframes, categories, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# Plot the average ratings per month for the specified year\n",
    "colors = ['dodgerblue', 'orange', 'green', 'purple', 'red']\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i, (category, average_ratings) in enumerate(average_ratings_2021.items()):\n",
    "    plt.plot(average_ratings['month'], average_ratings['rating'], color=colors[i], marker='o', label=category)\n",
    "\n",
    "min_month = min(min(avg['month']) for avg in average_ratings_2021.values() if not avg['month'].empty)\n",
    "max_month = max(max(avg['month']) for avg in average_ratings_2021.values() if not avg['month'].empty)\n",
    "\n",
    "min_y = min(min(avg['rating']) for avg in average_ratings_2021.values() if not avg['rating'].empty)\n",
    "max_y = max(max(avg['rating']) for avg in average_ratings_2021.values() if not avg['rating'].empty)\n",
    "\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Average Rating\")\n",
    "plt.title(f\"Average Ratings per Month in {year} by Category\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.xticks(range(1, 13), months)\n",
    "plt.ylim(min_y -0.1*min_y, max_y + 0.1*max_y)  # Adjust y-axis limits based on the dataset\n",
    "plt.grid(color='dimgray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Feature Engineering with Sentiment Scores and Ratings\n",
    "In the second task we had to choose between **two alternative methods** to combine text sentiment and review ratings to create a final sentiment score. Specifically the methods we were offered with are the following:\n",
    "1. Alternative 1: **Weighted Combination of Text Sentiment and Rating**, and\n",
    "2. Alternative 2: **Rating-Adjusted Sentiment**\n",
    "\n",
    "In this work we chose to work with the first alternative, thinking that it will suit better to extract the final sentiment score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER Download\n",
    "From the available options of extracting a base sentiment according to the review text, we prefer to work with **VADER**, a simple light weight sentiment classifier. This classifier takes as input a specific text, and outputs a score refering to its sentiment, specifically from **-1 (negative)** to **+1 (positive)**. Let's download the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the VADER classifier\n",
    "nltk.download('vader_lexicon')\n",
    "classifier = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Default Sentiment\n",
    "After downloading the classifier for sentiment extraction from text, let's test it on our data and create a new column containing the sentiment score for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    print(f\"Computing sentiment for {category}...\")\n",
    "    dataframes['review'][category]['base_sentiment'] = dataframes['review'][category]['cleaned_text'].apply(\n",
    "        lambda x: classifier.polarity_scores(str(x))['compound']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Rating\n",
    "Before we proceed into using the rating to produce the final sentiment we need to normalize the rating column to the range of (0, 1). This will be usefull for us if we want to cobmine it with the base sentiment that we extracted using VADER classifier. The formula that normalizes the rating is shown below:\n",
    "$$\n",
    "    Normalized\\_Rating = \\frac{Rating - 1}{4}\n",
    "$$\n",
    "We will apply this formula to receive the normalized rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    print(f\"Normalizing rating for {category}...\")\n",
    "    dataframes['review'][category]['normalized_rating'] = dataframes['review'][category]['rating'].apply(\n",
    "        lambda x: (x-1)/4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the final Sentiment Score\n",
    "Finally we will calculate the final sentiment score using the `base_sentiment` and `normalized_rating` columns that we have just computed. Specifically we will define two variables $w_1$ and $w_2$, that can be adjust to our preferences. This variables will be used in the final sentiment calculation as follows:\n",
    "$$\n",
    "    Final\\_Sentiment\\_Score = w_1 \\times Base\\_Sentiment + w_2 \\times Normalized\\_Rating\n",
    "$$\n",
    "Let's calculate it and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the weights of the final sentiment calculation\n",
    "w1 = 0.3\n",
    "w2 = 1.0 - w1\n",
    "\n",
    "# Calculate the final sentiment score for each category\n",
    "for category in categories:\n",
    "    print(f\"Computing final sentiment for {category}...\")\n",
    "    current_df = dataframes['review'][category]\n",
    "    current_df['final_sentiment'] = w1*current_df['base_sentiment'] + w1*current_df['normalized_rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the modified `All_Beauty` dataframe and how it is so far after all the previous additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['review']['All_Beauty'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the final sentiment score has been successfully computed and stored inside the `final_sentiment` column."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Conda (Data-Mining-venv)",
   "language": "python",
   "name": "data-mining-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
